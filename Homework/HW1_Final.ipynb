{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddcb38a2",
   "metadata": {},
   "source": [
    "# Homework 1 (Due Thursday, November 3rd, 2022 at 6:29pm PST)\n",
    "\n",
    "Every day late is -10%.\n",
    "\n",
    "You are a business analyst working for a major US toy retailer. A manager in the marketing department would like your help to build a classification model that will predict whether a review is positive or negative. use the `../datasets/good_amazon_toy_reviews.txt` and `../datasets/poor_amazon_toy_reviews.txt` datasets for this exercise.\n",
    "\n",
    "Combine the good and the poor datasets together.\n",
    "\n",
    "### Preprocessing and Regex (3pts)\n",
    "\n",
    "Perform the following cleanup steps:\n",
    "\n",
    "* There are malformed characters in the review text. For instance, notice the `&#34;` - these are examples of incorrectly decoded [HTML encodings](https://krypted.com/utilities/html-encoding-reference/).\n",
    "```\n",
    "\"amazing quality first of all, these cards are amazing proxies (but don't try to use em in &#34;official duels&#34; unless a judge is okay with it, if you have the real thing to show) and look amazing in your binder!\"\n",
    "```\n",
    "Please clean up all instances of these incorrect decodings.\n",
    "\n",
    "* Use **regular expressions to parse out and normalize all references to recipients (children, spouses, parents, etc.) and gift occasions (Christmas, birthdays, and anniversaries)**, and account for the possibility that people may spell words \"son\" / \"children\" / \"Christmas\" as both singular and plural, upper or lower-cased.\n",
    "\n",
    "### Vectorization (7pts)\n",
    "\n",
    "* with/without stopword removal\n",
    "* with 1) no stemming or lemmatization, 2) stemming, 3) lemmatization\n",
    "* using `TfIdfVectorizer` versus `CountVectorizer`\n",
    "* using `ngram` sizes of 1, 2, and 3\n",
    "\n",
    "Edit: Perform vectorization using above instructions. Print out the shape of the final vectorized datasets.\n",
    "\n",
    "**Submit everything as a new notebook and Slack direct message to me (Yu Chen) and the TAs the HW as an attachment.**\n",
    "\n",
    "**NOTE**: Name the notebook `lastname_firstname_HW1.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a7ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51874398",
   "metadata": {},
   "source": [
    "## Part1: Preprocessing and Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d78dbd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114917, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excellent!!!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Great quality wooden track (better than some ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my daughter loved it and i liked the price and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great item. Pictures pop thru and add detail a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was pleased with the product.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0                                     Excellent!!!\\n\n",
       "1  \"Great quality wooden track (better than some ...\n",
       "2  my daughter loved it and i liked the price and...\n",
       "3  Great item. Pictures pop thru and add detail a...\n",
       "4                  I was pleased with the product.\\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe & Concate the good and the poor datasets\n",
    "df1 = pd.DataFrame( open(\"../datasets/good_amazon_toy_reviews.txt\", \"r\"), columns=[\"review\"])\n",
    "df2 = pd.DataFrame( open(\"../datasets/poor_amazon_toy_reviews.txt\", \"r\"), columns=[\"review\"])\n",
    "df = pd.concat([df1, df2]).reset_index(drop = True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683f31fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to clean up the review\n",
    "def Cleanup(text):\n",
    "    # remove malformed characters in the review text\n",
    "    #cleaned_text = html.unescape(text).strip().lower()\n",
    "    cleaned_text = text.strip().lower()\n",
    "    cleaned_text = re.sub(r'(&#[0-9]+)|(%(\\w)+)', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'(\\<(br) \\/\\>)(\\<(br) \\/\\>)?', ' ', cleaned_text)\n",
    "    cleaned_text = re.sub('['+string.punctuation+']', '', cleaned_text)\n",
    "    \n",
    "    # Use regular expressions to parse out and normalize all references to recipients and gift occasions \n",
    "    cleaned_text = re.sub(r\"\\b((b(irth)?day)( party| parties)?|anniversar(y|ies)|(christ|x)mas|halloween( party| parties)?|thanksgiving|valentine's day|father's day|mother's day|housewarming( party| parties)?)\\b\", \n",
    "                        \"_OCCASION_\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\b(my |our )?([a-zA_Z0-9]+ ?(y(ea)?rs?(-| ?)(olds?)?|y\\.?o\\.?|months?(-| ?)olds?) )?(younger )?(older )?(twin )?(daughters?|sons?|child|children|kids?|husbands?|wifes?|(little )?girls?|(little )?boys?|mom|mother|dad|father|grand ?sons?|grand ?daughters?|grand ?child|grand ?children|grand ?kids?|girlfriend|boyfriend|honey|baby|babies|sisters?|brothers?|aunts?|uncles?|cousins?|fiances?|parents?|friends?|classmates?|co\\-?workers?|nieces?|nephews?)\\b\", \n",
    "                        \"_RECIPIENT_\", cleaned_text)\n",
    "    cleaned_text = re.sub(r\"\\b(my |our )([a-zA_Z0-9]+ (years?-olds?|years? ?olds?|yr\\.? olds?|yrs?|y\\.?o\\.?|months?-olds?|months? olds?))( and )?([a-zA_Z0-9]+ (years?-olds?|years? ?olds?|yr\\.? olds?|yrs?|y\\.?o\\.?|months?-olds?|months? olds?))?\\b\", \n",
    "                        \"_RECIPIENT_\", cleaned_text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b2a80b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"] = df.apply(lambda x: Cleanup(x[\"review\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5763f6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great quality wooden track better than some ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_RECIPIENT_ loved it and i liked the price and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great item pictures pop thru and add detail as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was pleased with the product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0                                          excellent\n",
       "1  great quality wooden track better than some ot...\n",
       "2  _RECIPIENT_ loved it and i liked the price and...\n",
       "3  great item pictures pop thru and add detail as...\n",
       "4                     i was pleased with the product"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1320746",
   "metadata": {},
   "source": [
    "## Part 2: Perform vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e62261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep nltk\n",
    "nltk_stopwords = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "# lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dabac2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reference: https://gist.github.com/gaurav5430/9fce93759eb2f6b1697883c3782f30de#file-nltk-lemmatize-sentences-py\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98bc09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(Stopword_Removal, Stemming, Lemmatization, text):\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # stopword removal\n",
    "    if Stopword_Removal == True:\n",
    "        word_remove_stopwords = []\n",
    "        for t in word_tokens:\n",
    "            if t in nltk_stopwords:\n",
    "                continue\n",
    "            word_remove_stopwords.append(t)\n",
    "        word_tokens = word_remove_stopwords\n",
    "    \n",
    "    # stemming \n",
    "    if Stemming == True:\n",
    "        word_tokens = [stemmer.stem(t) for t in word_tokens]\n",
    "    \n",
    "    # lemmatization\n",
    "    if Lemmatization == True:\n",
    "        word_tokens = [lemmatize_sentence(t) for t in word_tokens]\n",
    "        \n",
    "    cleaned_review = \" \".join(word_tokens)\n",
    "        \n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d16c1427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "def Vectorize(Vectorization, Ngram, text):\n",
    "    if Vectorization == 'COUNT':\n",
    "        vectorizer = CountVectorizer(ngram_range=Ngram)\n",
    "        X = vectorizer.fit_transform(text)\n",
    "        Count = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "        \n",
    "        return Count\n",
    "        \n",
    "    elif Vectorization == 'TFIDF':\n",
    "        vectorizer = TfidfVectorizer(ngram_range=Ngram)\n",
    "        corpus = list(text.values)\n",
    "        X = vectorizer.fit_transform(corpus)\n",
    "        terms = vectorizer.get_feature_names()\n",
    "        tf_idf = pd.DataFrame(X.toarray(), columns=terms)\n",
    "        \n",
    "        return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa42a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "instructions = [[False, False, False], [False, True, False], [False, False, True], [True, False, False],\n",
    "                [True, True, False], [True, False, True]]\n",
    "vectorize = [['COUNT', (1,1)], ['COUNT', (2,2)], ['COUNT', (3,3)], \n",
    "             ['TFIDF', (1,1)], ['TFIDF', (2,2)], ['TFIDF', (3,3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "237c3e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample for only 20,000 rows \n",
    "df2 = df.sample(2000)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afc49b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe(Stopword_Removal=False, Stemming=False, Lemmatization=False, Vectorization=COUNT, Ngram=(1, 1)) is (2000, 4366)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=False, Lemmatization=False, Vectorization=COUNT, Ngram=(2, 2)) is (2000, 28536)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=False, Lemmatization=False, Vectorization=COUNT, Ngram=(3, 3)) is (2000, 44157)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=False, Lemmatization=False, Vectorization=TFIDF, Ngram=(1, 1)) is (2000, 4366)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=False, Lemmatization=False, Vectorization=TFIDF, Ngram=(2, 2)) is (2000, 28536)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=False, Lemmatization=False, Vectorization=TFIDF, Ngram=(3, 3)) is (2000, 44157)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=True, Lemmatization=False, Vectorization=COUNT, Ngram=(1, 1)) is (2000, 4366)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=True, Lemmatization=False, Vectorization=COUNT, Ngram=(2, 2)) is (2000, 28536)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=True, Lemmatization=False, Vectorization=COUNT, Ngram=(3, 3)) is (2000, 44157)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=True, Lemmatization=False, Vectorization=TFIDF, Ngram=(1, 1)) is (2000, 4366)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=True, Lemmatization=False, Vectorization=TFIDF, Ngram=(2, 2)) is (2000, 28536)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=True, Lemmatization=False, Vectorization=TFIDF, Ngram=(3, 3)) is (2000, 44157)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=False, Lemmatization=True, Vectorization=COUNT, Ngram=(1, 1)) is (2000, 4311)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=False, Lemmatization=True, Vectorization=COUNT, Ngram=(2, 2)) is (2000, 27816)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=False, Lemmatization=True, Vectorization=COUNT, Ngram=(3, 3)) is (2000, 43837)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=False, Lemmatization=True, Vectorization=TFIDF, Ngram=(1, 1)) is (2000, 4310)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=False, Lemmatization=True, Vectorization=TFIDF, Ngram=(2, 2)) is (2000, 27816)\n",
      "Shape of dataframe(Stopword_Removal=False, Stemming=False, Lemmatization=True, Vectorization=TFIDF, Ngram=(3, 3)) is (2000, 43837)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=False, Lemmatization=False, Vectorization=COUNT, Ngram=(1, 1)) is (2000, 4219)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=False, Lemmatization=False, Vectorization=COUNT, Ngram=(2, 2)) is (2000, 22793)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=False, Lemmatization=False, Vectorization=COUNT, Ngram=(3, 3)) is (2000, 26654)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=False, Lemmatization=False, Vectorization=TFIDF, Ngram=(1, 1)) is (2000, 4219)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=False, Lemmatization=False, Vectorization=TFIDF, Ngram=(2, 2)) is (2000, 22793)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=False, Lemmatization=False, Vectorization=TFIDF, Ngram=(3, 3)) is (2000, 26654)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=True, Lemmatization=False, Vectorization=COUNT, Ngram=(1, 1)) is (2000, 4215)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=True, Lemmatization=False, Vectorization=COUNT, Ngram=(2, 2)) is (2000, 22785)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=True, Lemmatization=False, Vectorization=COUNT, Ngram=(3, 3)) is (2000, 26654)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=True, Lemmatization=False, Vectorization=TFIDF, Ngram=(1, 1)) is (2000, 4215)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=True, Lemmatization=False, Vectorization=TFIDF, Ngram=(2, 2)) is (2000, 22785)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=True, Lemmatization=False, Vectorization=TFIDF, Ngram=(3, 3)) is (2000, 26654)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=False, Lemmatization=True, Vectorization=COUNT, Ngram=(1, 1)) is (2000, 4215)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=False, Lemmatization=True, Vectorization=COUNT, Ngram=(2, 2)) is (2000, 22785)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=False, Lemmatization=True, Vectorization=COUNT, Ngram=(3, 3)) is (2000, 26654)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=False, Lemmatization=True, Vectorization=TFIDF, Ngram=(1, 1)) is (2000, 4215)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=False, Lemmatization=True, Vectorization=TFIDF, Ngram=(2, 2)) is (2000, 22785)\n",
      "Shape of dataframe(Stopword_Removal=True, Stemming=False, Lemmatization=True, Vectorization=TFIDF, Ngram=(3, 3)) is (2000, 26654)\n"
     ]
    }
   ],
   "source": [
    "for Stopword_Removal, Stemming, Lemmatization in instructions:\n",
    "    for Vectorization, Ngram in vectorize:\n",
    "        df2[\"review\"] = df2.apply(lambda x: Preprocess(Stopword_Removal, Stemming, Lemmatization, x[\"review\"]), axis=1)\n",
    "        vectorized_df = Vectorize(Vectorization, Ngram, df2[\"review\"])\n",
    "        print(f\"Shape of dataframe(Stopword_Removal={Stopword_Removal}, Stemming={Stemming}, Lemmatization={Lemmatization}, Vectorization={Vectorization}, Ngram={Ngram}) is {vectorized_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bd7ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.guru99.com/stemming-lemmatization-python-nltk.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
